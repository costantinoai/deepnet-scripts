{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from loss_fn import ContrastiveLoss\n",
    "from util_fns import *\n",
    "from dl_fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "log = False\n",
    "# stim_path = r'C:\\Users\\45027900\\Desktop\\NeuroFovea_PyTorch-main\\metamers'\n",
    "stim_path = r\"C:\\Users\\45027900\\Desktop\\cornet\\stimuli\\samediff\"\n",
    "# stim_path = r'C:\\Users\\45027900\\Desktop\\cornet\\stimuli\\no_transf'\n",
    "log_dir = r\"C:\\Users\\45027900\\Desktop\\cornet\\siamese_logs\"\n",
    "batch_sz = 56  # 6720 // 120\n",
    "# batch_sz = 24\n",
    "cycles = 1\n",
    "epochs = 100\n",
    "lr_min = 1e-3\n",
    "weight_decay = 1e-2\n",
    "fb = False\n",
    "fov_noise = False\n",
    "run_id = \"BIGGER_PER_KERNEL\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE DATALOADER\n",
    "# pairs = glob.glob(os.path.join(stim_path, '*.png'))\n",
    "# display_images([Image.open(i) for i in random.sample(pairs, 10)])\n",
    "\n",
    "dls = make_dls(stim_path, batch_sz, fov_noise)\n",
    "# print('\\nShowing first batch...')\n",
    "# dls.show_batch(max_n = 2)\n",
    "# plt.show()\n",
    "\n",
    "# train_loader = dls[0]\n",
    "# test_loader = dls[1]\n",
    "train_loader = dls.train\n",
    "test_loader = dls.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c690eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT NET\n",
    "class SiameseNetEncoderFB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetEncoderFB, self).__init__()\n",
    "\n",
    "        # V1 layers\n",
    "        self.V1_p = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                3, 64, kernel_size=7 * 2, stride=2, padding=7 // 2\n",
    "            ),  # + self.vfb,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # V2 layers\n",
    "        self.V2_p = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=3 // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # V4 layers\n",
    "        self.V4_p = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=3 // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # IT layers\n",
    "        self.IT_p = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=3 // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # V1 layers\n",
    "        self.V1_f = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=7 // 2),  # + self.vfb,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # V2 layers\n",
    "        self.V2_f = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=3 // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # V4 layers\n",
    "        self.V4_f = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=3 // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # IT layers\n",
    "        self.IT_f = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=3 // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # head\n",
    "        self.head = nn.Sequential(\n",
    "            AdaptiveConcatPool2d(),\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(\n",
    "                2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            ),\n",
    "            nn.Dropout(p=0.25, inplace=False),\n",
    "            nn.Linear(in_features=2048, out_features=512, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(\n",
    "                512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            ),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=512, out_features=2, bias=False),\n",
    "        )\n",
    "\n",
    "        self.fb = nn.Sequential(\n",
    "            nn.Conv2d(1024, 3, kernel_size=3, stride=1, padding=221),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp1 = inp[0]\n",
    "        inp2 = inp[1]\n",
    "        fov_inp = inp[2]\n",
    "\n",
    "        # perihperal 1\n",
    "        v1_p1 = self.V1_p(inp1)\n",
    "        v2_p1 = self.V2_p(v1_p1)\n",
    "        v4_p1 = self.V4_p(v2_p1)\n",
    "        vIT_p1 = self.IT_p(v4_p1)\n",
    "\n",
    "        # perihperal 1\n",
    "        v1_p2 = self.V1_p(inp2)\n",
    "        v2_p2 = self.V2_p(v1_p2)\n",
    "        v4_p2 = self.V4_p(v2_p2)\n",
    "        vIT_p2 = self.IT_p(v4_p2)\n",
    "\n",
    "        out_cat = torch.cat((vIT_p1, vIT_p2), 1)\n",
    "\n",
    "        # fovea\n",
    "        fb = self.fb(out_cat)\n",
    "        try:\n",
    "            v1_fov = self.V1_f(fb + fov_inp)\n",
    "        except:\n",
    "            v1_fov = self.V1_f(fov_inp)\n",
    "        v2_fov = self.V2_f(v1_fov)\n",
    "        v4_fov = self.V4_f(v2_fov)\n",
    "        vIT_fov = self.IT_f(v4_fov)\n",
    "\n",
    "        out_all = torch.cat((vIT_p1 + vIT_fov, vIT_p2 + vIT_fov), 1)\n",
    "        out = self.head(out_all)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22162fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetEncoderFB().cuda()\n",
    "net = init_weights(net)\n",
    "net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "\n",
    "params_to_update = net.parameters()\n",
    "optimizer = optim.Adam(params_to_update, lr=lr_min, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b8140",
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TRAIN/TEST\n",
    "if log:\n",
    "    timestamp = datetime.now().strftime(\"%d-%b-%Y_%H-%M-%S\")\n",
    "    run_name = f\"{timestamp}_{run_id}_fb-{fb}_fovnoise-{fov_noise}_CrossEntLoss_{os.path.normpath(stim_path).split(os.sep)[-1]}_adam_bs-{batch_sz}_lr-{lr_min}_{cycles}x{epochs}\"\n",
    "    path = os.path.join(log_dir, run_name)\n",
    "    logger = start_logger(path)\n",
    "    shutil.copy(\n",
    "        r\"C:\\Users\\45027900\\Desktop\\cornet\\project\\main.py\",\n",
    "        os.path.join(path, \"main.py\"),\n",
    "    )\n",
    "else:\n",
    "    path = \"\"\n",
    "\n",
    "print(\"\\nTrain/Test started!\")\n",
    "# weights = net.module.V1[0].weight.data.cpu()\n",
    "# plot_filters_multi_channel(weights, path)\n",
    "\n",
    "for cycle in range(cycles):\n",
    "    tr_loss = []\n",
    "    tr_acc = []\n",
    "    te_loss = []\n",
    "    te_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # TRAIN\n",
    "        net.train()\n",
    "\n",
    "        tr_running_loss = 0.0\n",
    "        tr_correct = 0\n",
    "        tr_total = 0\n",
    "        start = time.time()\n",
    "        for (inputs, labels) in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = net(inputs)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_running_loss += loss.item()\n",
    "            tr_total += labels.size(0)\n",
    "            tr_correct += (pred == labels).sum().item()\n",
    "\n",
    "        tr_loss.append(tr_running_loss)\n",
    "        tr_acc.append(100 * tr_correct / tr_total)\n",
    "\n",
    "        # TEST\n",
    "        net.eval()\n",
    "\n",
    "        te_running_loss = 0.0\n",
    "        te_correct = 0\n",
    "        te_total = 0\n",
    "        cf_pred = []\n",
    "        cf_y = []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels) in test_loader:\n",
    "                out = net(inputs)\n",
    "                _, pred = torch.max(out, 1)\n",
    "                loss = criterion(out, labels)\n",
    "                te_running_loss += loss.item()\n",
    "                te_total += labels.size(0)\n",
    "                te_correct += (pred == labels).sum().item()\n",
    "                cf_y += labels.cpu().detach().tolist()\n",
    "                cf_pred += pred.cpu().detach().tolist()\n",
    "\n",
    "            te_acc.append(100 * te_correct / te_total)\n",
    "            te_loss.append(te_running_loss)\n",
    "            end = time.time() - start\n",
    "            log_msg = f\"%5d / %5d TRAIN/TEST losses: \\t %.8f \\t %.8f \\t\\t acc: \\t %.2f %% \\t %.2f %% \\t\\t time: {round(end,3)}\" % (\n",
    "                cycle + 1,\n",
    "                epoch + 1,\n",
    "                tr_running_loss,\n",
    "                te_running_loss,\n",
    "                100 * tr_correct / tr_total,\n",
    "                100 * te_correct / te_total,\n",
    "            )\n",
    "            print(log_msg)\n",
    "            if log:\n",
    "                logger.info(log_msg)\n",
    "    make_cf(cf_y, cf_pred, cycle, epoch, path)\n",
    "    plot_losses(tr_loss, te_loss, cycle, epoch, path)\n",
    "    plot_acc(tr_acc, te_acc, cycle, epoch, path)\n",
    "    if log:\n",
    "        filename = f\"{datetime.now().strftime('%d-%b-%Y_%H-%M-%S')}_{cycle+1}x{epoch+1}_trloss-{str(round(tr_running_loss, 5)).split('.')[-1]}_teacc-{str(round(te_correct / te_total, 4)).split('.')[-1]}\"\n",
    "        torch.save(net.state_dict(), os.path.join(path, filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
